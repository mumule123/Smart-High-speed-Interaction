#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¾å¤‡æ“ä½œæ£€æµ‹ç³»ç»Ÿ - ç®€åŒ–ç‰ˆæœ¬
ä½¿ç”¨å›ºå®šåæ ‡è€Œä¸è¿›è¡Œè®¾å¤‡è¯†åˆ«ï¼Œç»“åˆäººä½“å§¿æ€è¯†åˆ«åˆ†ææ“ä½œè¡Œä¸º
"""

import os
import cv2
import numpy as np
import pandas as pd
from ultralytics import YOLO
import argparse
from collections import defaultdict
import json  # ä»…ç”¨äºä¿å­˜åˆ†ææŠ¥å‘Š
from datetime import datetime
import math
from PIL import Image, ImageDraw, ImageFont

# å¯¼å…¥é…ç½®ç®¡ç†å™¨
from config_manager import ConfigManager


class OperationDetector:
    def __init__(self, project_root, config_file_path):
        """
        åˆå§‹åŒ–æ“ä½œæ£€æµ‹å™¨
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
            config_file_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.project_root = project_root
        
        # åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆä½¿ç”¨é…ç½®ç®¡ç†å™¨ï¼‰
        config_manager = ConfigManager()
        self.config = config_manager.load_config(config_file_path)
        
        # è®¾å¤‡ç±»åˆ«æ˜ å°„ï¼ˆä»é…ç½®ä¸­è·å–ï¼‰
        self.all_classes = self.config['all_classes']
        
        # æ“ä½œåˆ¤æ–­é˜ˆå€¼ï¼ˆåƒç´ è·ç¦»ï¼Œä»é…ç½®æ–‡ä»¶è·å–ï¼‰
        self.operation_threshold = self.config['thresholds']['operation_distance_threshold']
        
        # åŠ è½½æ¨¡å‹ï¼ˆåªåŠ è½½å§¿æ€è¯†åˆ«æ¨¡å‹ï¼Œä¸åŠ è½½ç›®æ ‡æ£€æµ‹æ¨¡å‹ï¼‰
        self.load_models()
        
        # æ“ä½œè®°å½•
        self.operation_records = defaultdict(list)
        
        # åŠ è½½ä¸­æ–‡å­—ä½“
        self.load_chinese_font()
    
    def get_target_devices_from_detection_info(self):
        """
        ä» detection_info_254.txt æ–‡ä»¶ä¸­è·å–ç›®æ ‡è®¾å¤‡ä¿¡æ¯
        åŒ…æ‹¬è®¾å¤‡IDã€åæ ‡ç­‰ä¿¡æ¯ï¼Œç‰¹åˆ«å¤„ç†é«˜å‹éš”ç¦»å¼€å…³çš„4ä¸ªè®¾å¤‡
        """
        detection_info_file = os.path.join(self.project_root, self.config['file_paths']['fixed_coords_file'])
        target_devices = []
        
        try:
            with open(detection_info_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
                i = 0
                while i < len(lines):
                    line = lines[i].strip()
                    
                    if line.startswith('ç›®æ ‡'):
                        device_info = {}
                        i += 1
                        
                        # è§£æç±»åˆ«ä¿¡æ¯
                        if i < len(lines) and 'ç±»åˆ«:' in lines[i]:
                            class_line = lines[i].strip()
                            if '(' in class_line and 'ID:' in class_line:
                                class_name = class_line.split('ç±»åˆ«:')[1].split('(')[0].strip()
                                class_id_str = class_line.split('ID:')[1].split(')')[0].strip()
                                device_info['class_name'] = class_name
                                device_info['class_id'] = int(class_id_str)
                            i += 1
                        
                        # è§£æç½®ä¿¡åº¦
                        if i < len(lines) and 'ç½®ä¿¡åº¦:' in lines[i]:
                            conf_line = lines[i].strip()
                            confidence = float(conf_line.split('ç½®ä¿¡åº¦:')[1].strip())
                            device_info['confidence'] = confidence
                            i += 1
                        
                        # è·³è¿‡è¾¹æ¡†åæ ‡æ ‡é¢˜è¡Œ
                        if i < len(lines) and 'è¾¹æ¡†åæ ‡:' in lines[i]:
                            i += 1
                        
                        # è§£æåæ ‡ä¿¡æ¯ 
                        bbox_coords = {}
                        for _ in range(4):
                            if i < len(lines):
                                coord_line = lines[i].strip()
                                if '(' in coord_line and ')' in coord_line:
                                    coord_str = coord_line.split('(')[1].split(')')[0]
                                    x, y = map(float, coord_str.split(','))
                                    if 'å·¦ä¸Šè§’' in coord_line:
                                        bbox_coords['x1'] = x
                                        bbox_coords['y1'] = y
                                    elif 'å³ä¸‹è§’' in coord_line:
                                        bbox_coords['x2'] = x
                                        bbox_coords['y2'] = y
                                i += 1
                        
                        # è§£æä¸­å¿ƒç‚¹åæ ‡
                        if i < len(lines) and 'ä¸­å¿ƒç‚¹åæ ‡:' in lines[i]:
                            center_line = lines[i].strip()
                            if '(' in center_line and ')' in center_line:
                                coord_str = center_line.split('(')[1].split(')')[0]
                                center_x, center_y = map(float, coord_str.split(','))
                                device_info['center_x'] = center_x
                                device_info['center_y'] = center_y
                            i += 1
                        
                        # å¦‚æœè§£æåˆ°äº†æœ‰æ•ˆä¿¡æ¯ï¼Œæ·»åŠ åˆ°åˆ—è¡¨ä¸­
                        if 'class_id' in device_info and 'center_x' in device_info:
                            if 'x1' in bbox_coords and 'x2' in bbox_coords:
                                device_info['bbox'] = [bbox_coords['x1'], bbox_coords['y1'], 
                                                     bbox_coords['x2'], bbox_coords['y2']]
                            else:
                                estimation_size = self.config['device_detection']['missing_device_estimation_size']
                                device_info['bbox'] = [device_info['center_x'] - estimation_size, device_info['center_y'] - estimation_size,
                                                     device_info['center_x'] + estimation_size, device_info['center_y'] + estimation_size]
                            
                            target_devices.append(device_info)
                    else:
                        i += 1
            
            print(f"âœ“ ä»æ£€æµ‹ä¿¡æ¯æ–‡ä»¶åŠ è½½ç›®æ ‡è®¾å¤‡: å…± {len(target_devices)} ä¸ªè®¾å¤‡")
            
            # ç»Ÿè®¡æ¯ç§è®¾å¤‡ç±»å‹çš„æ•°é‡ï¼Œç‰¹åˆ«æ ‡è®°é«˜å‹éš”ç¦»å¼€å…³
            device_count = {}
            for device in target_devices:
                class_name = device['class_name']
                class_id = device['class_id']
                key = f"{class_name} (ID: {class_id})"
                device_count[key] = device_count.get(key, 0) + 1
            
            for device_type, count in device_count.items():
                if count > 1:
                    print(f"  - {device_type}: {count} ä¸ªè®¾å¤‡")
                else:
                    print(f"  - {device_type}: {count} ä¸ªè®¾å¤‡")
            
        except Exception as e:
            print(f"è­¦å‘Š: ä»æ£€æµ‹ä¿¡æ¯æ–‡ä»¶åŠ è½½ç›®æ ‡è®¾å¤‡å¤±è´¥ {e}")
            
        return target_devices
    
    def load_chinese_font(self):
        """åŠ è½½ä¸­æ–‡å­—ä½“"""
        try:
            # ä¸­æ–‡å­—ä½“è·¯å¾„
            font_path = os.path.join(self.project_root, self.config['font_settings']['chinese_font_path'])
            if os.path.exists(font_path):
                font_size_normal = self.config['font_settings']['font_size_normal']
                font_size_small = self.config['font_settings']['font_size_small']
                self.chinese_font = ImageFont.truetype(font_path, font_size_normal)
                self.chinese_font_small = ImageFont.truetype(font_path, font_size_small)
                print(f"âœ“ ä¸­æ–‡å­—ä½“åŠ è½½æˆåŠŸ: {font_path}")
            else:
                print(f"è­¦å‘Š: ä¸­æ–‡å­—ä½“æ–‡ä»¶ä¸å­˜åœ¨: {font_path}")
                self.chinese_font = ImageFont.load_default()
                self.chinese_font_small = ImageFont.load_default()
        except Exception as e:
            print(f"è­¦å‘Š: ä¸­æ–‡å­—ä½“åŠ è½½å¤±è´¥ {e}, ä½¿ç”¨é»˜è®¤å­—ä½“")
            self.chinese_font = ImageFont.load_default()
            self.chinese_font_small = ImageFont.load_default()
    
    def draw_chinese_text(self, image, text, position, font, color=(255, 255, 255)):
        """
        åœ¨å›¾åƒä¸Šç»˜åˆ¶ä¸­æ–‡æ–‡æœ¬
        Args:
            image: OpenCVå›¾åƒ (BGRæ ¼å¼)
            text: è¦ç»˜åˆ¶çš„æ–‡æœ¬
            position: æ–‡æœ¬ä½ç½® (x, y)
            font: PILå­—ä½“å¯¹è±¡
            color: æ–‡æœ¬é¢œè‰² (B, G, R)
        Returns:
            ç»˜åˆ¶äº†æ–‡æœ¬çš„å›¾åƒ
        """
        # å°†OpenCVå›¾åƒè½¬æ¢ä¸ºPILå›¾åƒ
        pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(pil_image)
        
        # ç»˜åˆ¶æ–‡æœ¬ (PILä½¿ç”¨RGBæ ¼å¼)
        rgb_color = (color[2], color[1], color[0])  # BGRè½¬RGB
        draw.text(position, text, font=font, fill=rgb_color)
        
        # è½¬æ¢å›OpenCVæ ¼å¼
        return cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
        
    def load_models(self):
        """åªåŠ è½½å§¿æ€è¯†åˆ«æ¨¡å‹ï¼Œä¸åŠ è½½ç›®æ ‡æ£€æµ‹æ¨¡å‹"""
        try:
            # åŠ è½½å§¿æ€è¯†åˆ«æ¨¡å‹
            pose_model_path = os.path.join(self.project_root, self.config['models']['pose_model_path'])
            self.pose_model = YOLO(pose_model_path)
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def get_hand_keypoints(self, keypoints):
        """
        ä»YOLO poseæ£€æµ‹ç»“æœä¸­æå–æ‰‹éƒ¨å…³é”®ç‚¹
        è¿”å›å·¦å³æ‰‹è…•å’Œæ‰‹è‚˜çš„åæ ‡
        """
        # YOLO poseå…³é”®ç‚¹ç´¢å¼•ï¼ˆCOCOæ ¼å¼ï¼‰- ä»é…ç½®ä¸­è·å–
        keypoint_indices = self.config['keypoint_indices']
        
        hand_points = {}
        
        for point_name, index in keypoint_indices.items():
            if index < len(keypoints):
                x, y, confidence = keypoints[index]
                # ä½¿ç”¨é…ç½®ä¸­çš„ç½®ä¿¡åº¦é˜ˆå€¼
                keypoint_threshold = self.config['thresholds']['keypoint_confidence_threshold']
                if confidence > keypoint_threshold:
                    hand_points[point_name] = {'x': float(x), 'y': float(y), 'confidence': float(confidence)}
                else:
                    hand_points[point_name] = {'x': None, 'y': None, 'confidence': float(confidence)}
            else:
                hand_points[point_name] = {'x': None, 'y': None, 'confidence': 0.0}
        
        return hand_points
    
    def calculate_weighted_hand_center(self, hand_points):
        """
        ä½¿ç”¨åŠ æƒå…¬å¼è®¡ç®—æ‰‹éƒ¨ä¸­å¿ƒç‚¹
        å…¬å¼: ä¸­å¿ƒç‚¹ä½ç½® = 0.35*left_wrist + 0.35*right_wrist + 0.15*right_elbow + 0.15*left_elbow
        """
        left_wrist = hand_points['left_wrist']
        right_wrist = hand_points['right_wrist']
        left_elbow = hand_points['left_elbow']
        right_elbow = hand_points['right_elbow']
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆç‚¹æ¥è®¡ç®—ä¸­å¿ƒç‚¹
        valid_points = []
        weights = []
        
        # ä»é…ç½®æ–‡ä»¶è·å–æƒé‡
        weights_config = self.config['hand_center_weights']
        
        if left_wrist['x'] is not None and left_wrist['y'] is not None:
            valid_points.append((left_wrist['x'], left_wrist['y']))
            weights.append(weights_config['left_wrist_weight'])
            
        if right_wrist['x'] is not None and right_wrist['y'] is not None:
            valid_points.append((right_wrist['x'], right_wrist['y']))
            weights.append(weights_config['right_wrist_weight'])
            
        if left_elbow['x'] is not None and left_elbow['y'] is not None:
            valid_points.append((left_elbow['x'], left_elbow['y']))
            weights.append(weights_config['left_elbow_weight'])
            
        if right_elbow['x'] is not None and right_elbow['y'] is not None:
            valid_points.append((right_elbow['x'], right_elbow['y']))
            weights.append(weights_config['right_elbow_weight'])
        
        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆç‚¹ï¼Œè¿”å›None
        if len(valid_points) == 0:
            return {'x': None, 'y': None}
        
        # æ ¹æ®æœ‰æ•ˆç‚¹é‡æ–°è®¡ç®—æƒé‡ï¼ˆç¡®ä¿æƒé‡å’Œä¸º1ï¼‰
        total_weight = sum(weights)
        normalized_weights = [w / total_weight for w in weights]
        
        # è®¡ç®—åŠ æƒä¸­å¿ƒç‚¹
        center_x = sum(p[0] * w for p, w in zip(valid_points, normalized_weights))
        center_y = sum(p[1] * w for p, w in zip(valid_points, normalized_weights))
        
        return {'x': float(center_x), 'y': float(center_y)}
    
    
    
    def calculate_center_point(self, points):
        """è®¡ç®—å¤šä¸ªç‚¹çš„ä¸­å¿ƒä½ç½®ï¼Œæ”¯æŒåˆ—è¡¨æˆ–å­—å…¸è¾“å…¥"""
        # å¦‚æœæ˜¯å­—å…¸ï¼ˆå¦‚ hand_pointsï¼‰ï¼Œå–å…¶ values
        if isinstance(points, dict):
            points = list(points.values())
        valid_points = []
        for point in points:
            if isinstance(point, dict) and point.get('x') is not None and point.get('y') is not None:
                valid_points.append((point['x'], point['y']))
        if len(valid_points) == 0:
            return {'x': None, 'y': None}
        center_x = sum(p[0] for p in valid_points) / len(valid_points)
        center_y = sum(p[1] for p in valid_points) / len(valid_points)
        return {'x': float(center_x), 'y': float(center_y)}
    
    
    
    
    def calculate_distance(self, point1, point2):
        """è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„æ¬§å‡ é‡Œå¾—è·ç¦»"""
        if (point1['x'] is None or point1['y'] is None or 
            point2['x'] is None or point2['y'] is None):
            return float('inf')
        
        dx = point1['x'] - point2['x']
        dy = point1['y'] - point2['y']
        return math.sqrt(dx * dx + dy * dy)
    
    def detect_objects(self, frame, frame_number=0, timestamp=0.0):
        """
        ç›´æ¥ä½¿ç”¨å›ºå®šåæ ‡ä½œä¸ºè®¾å¤‡ä½ç½®ï¼Œä¸è¿›è¡Œå®é™…çš„YOLOæ£€æµ‹
        
        Args:
            frame: è¾“å…¥å›¾åƒå¸§
            frame_number: å½“å‰å¸§å·
            timestamp: å½“å‰æ—¶é—´æˆ³(ç§’)ï¼ˆç”¨äºç¼“å­˜ç®¡ç†ï¼‰
            
        Returns:
            list: æ£€æµ‹åˆ°çš„è®¾å¤‡åˆ—è¡¨
        """
        # ç›´æ¥ä»å›ºå®šåæ ‡æ–‡ä»¶ä¸­è·å–æ‰€æœ‰è®¾å¤‡ä¿¡æ¯
        target_device_info = self.get_target_devices_from_detection_info()
        detected_objects = []
        
        # ä½¿ç”¨å›ºå®šåæ ‡ç½®ä¿¡åº¦
        fixed_confidence = self.config['thresholds']['fixed_coordinate_confidence']
        
        print(f"ï¿½ å¸§ {frame_number}: ä½¿ç”¨å›ºå®šåæ ‡åŠ è½½æ‰€æœ‰è®¾å¤‡")
        
        # ç›´æ¥å°†æ‰€æœ‰å›ºå®šåæ ‡è®¾å¤‡æ·»åŠ åˆ°æ£€æµ‹ç»“æœä¸­
        for device in target_device_info:
            detected_objects.append({
                'class_id': device['class_id'],
                'class_name': device['class_name'],
                'center': {'x': device['center_x'], 'y': device['center_y']},
                'bbox': device['bbox'],
                'confidence': fixed_confidence,
                'source': 'fixed_coordinates'  # æ ‡è®°æ¥æºä¸ºå›ºå®šåæ ‡
            })
        
        print(f"ï¿½ å¸§ {frame_number}: å›ºå®šåæ ‡æ¨¡å¼åŠ è½½äº† {len(detected_objects)} ä¸ªè®¾å¤‡")
        
        return detected_objects
    
    def detect_poses(self, frame):
        """ä½¿ç”¨å§¿æ€è¯†åˆ«æ¨¡å‹æ£€æµ‹äººä½“å§¿æ€"""
        results = self.pose_model(frame)
        detected_persons = []
        
        for r in results:
            if r.keypoints is not None:
                for i, keypoints in enumerate(r.keypoints.data):
                    # æå–æ‰‹éƒ¨å…³é”®ç‚¹
                    hand_points = self.get_hand_keypoints(keypoints)
                    
                    # ä½¿ç”¨åŠ æƒå…¬å¼è®¡ç®—æ‰‹éƒ¨ä¸­å¿ƒç‚¹
                    # å…¬å¼å‚æ•°ä»é…ç½®æ–‡ä»¶è·å–  2é’Ÿé€‰æ‹©ï¼Œä½¿ç”¨åŠ æƒæˆ–è€…ç›´æ¥å¹³å‡
                    hand_center = self.calculate_weighted_hand_center(hand_points)
                    
                    if hand_center['x'] is not None:  # åªæœ‰å½“èƒ½è®¡ç®—å‡ºæ‰‹éƒ¨ä¸­å¿ƒæ—¶æ‰æ·»åŠ 
                        detected_persons.append({
                            'person_id': i,
                            'hand_keypoints': hand_points,
                            'hand_center': hand_center
                        })
        
        return detected_persons
    
    def analyze_operations(self, persons, objects, frame_number, timestamp):
        """åˆ†æäººå‘˜æ“ä½œè¡Œä¸º"""
        operations = []
        
        for person in persons:
            if person['hand_center']['x'] is None:
                continue
                
            min_distance = float('inf')
            closest_object = None
            
            # æ‰¾åˆ°ç¦»æ‰‹éƒ¨ä¸­å¿ƒæœ€è¿‘çš„ç‰©ä½“
            for obj in objects:
                distance = self.calculate_distance(person['hand_center'], obj['center'])
                if distance < min_distance:
                    min_distance = distance
                    closest_object = obj
            
            # å¦‚æœè·ç¦»å°äºé˜ˆå€¼ï¼Œè®¤ä¸ºæ­£åœ¨æ“ä½œ
            if closest_object and min_distance <= self.operation_threshold:
                operation = {
                    'frame_number': frame_number,
                    'timestamp': timestamp,
                    'person_id': person['person_id'],
                    'device_class_id': closest_object['class_id'],
                    'device_name': closest_object['class_name'],
                    'distance': min_distance,
                    'hand_center': person['hand_center'],
                    'device_center': closest_object['center'],
                    'device_confidence': closest_object['confidence']
                }
                operations.append(operation)
                
                # è®°å½•æ“ä½œå†å²
                self.operation_records[closest_object['class_id']].append(operation)
        
        return operations
    
    def draw_annotations(self, frame, objects, persons, operations):
        """åœ¨å¸§ä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœå’Œæ“ä½œåˆ†æ"""
        annotated_frame = frame.copy()
        
        # ç»˜åˆ¶æ£€æµ‹åˆ°çš„ç‰©ä½“
        for obj in objects:
            center_x, center_y = int(obj['center']['x']), int(obj['center']['y'])
            
            # è®¾å¤‡ä¸­å¿ƒç‚¹é¢œè‰²å’ŒåŠå¾„ä»é…ç½®è·å–
            center_color = tuple(self.config['visualization']['device_center_color'])
            center_radius = self.config['visualization']['device_center_radius']
            
            # ç»˜åˆ¶ä¸­å¿ƒç‚¹
            cv2.circle(annotated_frame, (center_x, center_y), center_radius, center_color, -1)
        
        # ç»˜åˆ¶äººä½“æ‰‹éƒ¨å…³é”®ç‚¹
        hand_colors = self.config['hand_colors']
        keypoint_radius = self.config['visualization']['keypoint_radius']
        
        for person in persons:
            # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹
            for point_name, point_data in person['hand_keypoints'].items():
                if point_data['x'] is not None and point_data['y'] is not None:
                    x, y = int(point_data['x']), int(point_data['y'])
                    color = tuple(hand_colors.get(point_name, [255, 255, 255]))
                    cv2.circle(annotated_frame, (x, y), keypoint_radius, color, -1)
                    cv2.putText(annotated_frame, point_name[:5], (x+8, y-8), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.3, color, 1)
            
            # ç»˜åˆ¶æ‰‹éƒ¨ä¸­å¿ƒç‚¹
            if person['hand_center']['x'] is not None:
                center_x = int(person['hand_center']['x'])
                center_y = int(person['hand_center']['y'])
                hand_center_color = tuple(self.config['visualization']['hand_center_color'])
                hand_center_radius = self.config['visualization']['hand_center_radius']
                cv2.circle(annotated_frame, (center_x, center_y), hand_center_radius, hand_center_color, -1)  # çº¢è‰²
                cv2.putText(annotated_frame, f'P{person["person_id"]}', (center_x+12, center_y-12), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
        
        # ç»˜åˆ¶æ“ä½œè¿çº¿å’Œæ ‡æ³¨
        for op in operations:
            hand_x = int(op['hand_center']['x'])
            hand_y = int(op['hand_center']['y'])
            device_x = int(op['device_center']['x'])
            device_y = int(op['device_center']['y'])
            
            # ç»˜åˆ¶è¿çº¿
            line_color = tuple(self.config['visualization']['operation_line_color'])
            line_thickness = self.config['visualization']['operation_line_thickness']
            cv2.line(annotated_frame, (hand_x, hand_y), (device_x, device_y), line_color, line_thickness)
            
            # æ ‡æ³¨æ“ä½œä¿¡æ¯ï¼ˆä½¿ç”¨ä¸­æ–‡å­—ä½“ï¼‰
            mid_x = (hand_x + device_x) // 2
            mid_y = (hand_y + device_y) // 2
            operation_text = f"æ“ä½œä¸­: {op['device_name']}"
            distance_text = f"è·ç¦»: {op['distance']:.1f}px"
            
            # ä½¿ç”¨ä¸­æ–‡å­—ä½“ç»˜åˆ¶æ“ä½œä¿¡æ¯
            text_color = tuple(self.config['visualization']['operation_text_color'])
            annotated_frame = self.draw_chinese_text(
                annotated_frame, operation_text, (mid_x, mid_y-25), self.chinese_font_small, text_color
            )
            annotated_frame = self.draw_chinese_text(
                annotated_frame, distance_text, (mid_x, mid_y+5), self.chinese_font_small, text_color
            )
        
        return annotated_frame
    
    def process_video(self, video_path, output_dir):
        """å¤„ç†è§†é¢‘æ–‡ä»¶"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # æ‰“å¼€è§†é¢‘æ–‡ä»¶
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # éªŒè¯å¸§ç‡æœ‰æ•ˆæ€§ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä½œä¸ºå›é€€
        if fps <= 0 or fps > 240:
            original_fps = fps
            fps = self.config['video_processing']['fps_assumption']
            print(f"âš ï¸  è§†é¢‘å¸§ç‡æ— æ•ˆ ({original_fps} FPS)ï¼Œä½¿ç”¨é…ç½®ä¸­çš„å›é€€å¸§ç‡: {fps} FPS")
        
        # ä¿å­˜å®é™…ä½¿ç”¨çš„å¸§ç‡ä¾›åç»­è®¡ç®—ä½¿ç”¨
        self.actual_fps = fps
        
        print(f"è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps} FPS, å…± {total_frames} å¸§")
        
        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        output_video_path = os.path.join(output_dir, 'operation_analysis.mp4')
        out_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))
        
        # æ•°æ®è®°å½•
        all_frame_data = []
        frame_count = 0
        
        print("å¼€å§‹å¤„ç†è§†é¢‘...")
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            timestamp = frame_count / fps
            
            if frame_count % 30 == 0:  # æ¯30å¸§æ˜¾ç¤ºè¿›åº¦
                print(f"å¤„ç†è¿›åº¦: {frame_count}/{total_frames} ({frame_count/total_frames*100:.1f}%)")
            
            # æ£€æµ‹ç‰©ä½“å’Œäººä½“å§¿æ€
            detected_objects = self.detect_objects(frame, frame_count, timestamp)
            detected_persons = self.detect_poses(frame)
            
            # åˆ†ææ“ä½œè¡Œä¸º
            operations = self.analyze_operations(detected_persons, detected_objects, frame_count, timestamp)
            
            # ç»˜åˆ¶æ ‡æ³¨
            annotated_frame = self.draw_annotations(frame, detected_objects, detected_persons, operations)
            
            # å†™å…¥è§†é¢‘
            out_video.write(annotated_frame)
            
            # è®°å½•å¸§æ•°æ®
            frame_data = {
                'frame_number': frame_count,
                'timestamp': timestamp,
                'detected_objects': detected_objects,
                'detected_persons': detected_persons,
                'operations': operations
            }
            all_frame_data.append(frame_data)
        
        cap.release()
        out_video.release()
        
        print(f"è§†é¢‘å¤„ç†å®Œæˆï¼å…±å¤„ç† {frame_count} å¸§")
        print(f"æ ‡æ³¨è§†é¢‘ä¿å­˜ä¸º: {output_video_path}")
        
        # ä¿å­˜åˆ†æç»“æœ
        self.save_analysis_results(all_frame_data, output_dir)
        
        return all_frame_data
    
    def save_analysis_results(self, frame_data, output_dir):
        """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        
        # 1. è®¡ç®—æ¯ä¸ªè®¾å¤‡çš„æ“ä½œæ—¶é—´ç»Ÿè®¡
        device_operation_stats = self.calculate_operation_time_stats()
        
        # åˆ›å»ºè®¾å¤‡IDåˆ°è®¾å¤‡åç§°çš„æ˜ å°„ï¼ˆä»å›ºå®šåæ ‡æ–‡ä»¶ä¸­è·å–ï¼‰
        target_devices = self.get_target_devices_from_detection_info()
        device_id_to_name = {}
        for device in target_devices:
            device_id_to_name[device['class_id']] = device['class_name']
        
        stats_df = pd.DataFrame([
            {
                'device_class_id': device_id,
                'device_name': device_id_to_name.get(device_id, f"Unknown_{device_id}"),
                'total_operation_frames': stats['total_frames'],
                'total_operation_time_seconds': stats['total_time'],
                'operation_episodes': stats['episodes'],
                'average_distance': stats['avg_distance']
            }
            for device_id, stats in device_operation_stats.items()
        ])
        
        stats_csv_path = os.path.join(output_dir, 'device_operation_stats.csv')
        stats_df.to_csv(stats_csv_path, index=False, encoding='utf-8')
        print(f"è®¾å¤‡æ“ä½œç»Ÿè®¡ä¿å­˜ä¸º: {stats_csv_path}")
        
        # 2. ä¿å­˜å®Œæ•´çš„åˆ†ææŠ¥å‘Š
        report = {
            'analysis_info': {
                'total_frames': len(frame_data),
                'total_duration_seconds': frame_data[-1]['timestamp'] if frame_data else 0,
                'operation_threshold_pixels': self.operation_threshold,
                'device_classes': device_id_to_name,
                'analysis_timestamp': datetime.now().isoformat()
            },
            'device_operation_summary': device_operation_stats
        }
        
        report_path = os.path.join(output_dir, 'analysis_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"åˆ†ææŠ¥å‘Šä¿å­˜ä¸º: {report_path}")
        
        # 4. æ‰“å°æ‘˜è¦ä¿¡æ¯
        print("\n" + "="*50)
        print("æ“ä½œåˆ†ææ‘˜è¦")
        print("="*50)
        
        if device_operation_stats:
            print("\nå„è®¾å¤‡æ“ä½œæ—¶é—´ç»Ÿè®¡:")
            for device_id, stats in device_operation_stats.items():
                device_name = device_id_to_name.get(device_id, f"Unknown_{device_id}")
                print(f"  {device_name}:")
                print(f"    - æ“ä½œæ—¶é—´: {stats['total_time']:.2f} ç§’")
                print(f"    - æ“ä½œå¸§æ•°: {stats['total_frames']} å¸§")
                print(f"    - æ“ä½œæ¬¡æ•°: {stats['episodes']} æ¬¡")
                print(f"    - å¹³å‡è·ç¦»: {stats['avg_distance']:.2f} åƒç´ ")
        else:
            print("æœªæ£€æµ‹åˆ°ä»»ä½•æ“ä½œè¡Œä¸º")
    
    def calculate_operation_time_stats(self):
        """è®¡ç®—æ¯ä¸ªè®¾å¤‡çš„æ“ä½œæ—¶é—´ç»Ÿè®¡"""
        stats = {}
        
        for device_id, operations in self.operation_records.items():
            if not operations:
                continue
            
            # æŒ‰æ—¶é—´æ’åº
            operations_sorted = sorted(operations, key=lambda x: x['timestamp'])
            
            # è®¡ç®—è¿ç»­æ“ä½œçš„æ—¶é—´æ®µ
            episodes = []
            current_episode = [operations_sorted[0]]
            
            for i in range(1, len(operations_sorted)):
                # å¦‚æœä¸¤ä¸ªæ“ä½œä¹‹é—´çš„æ—¶é—´é—´éš”å°äº2ç§’ï¼Œè®¤ä¸ºæ˜¯è¿ç»­æ“ä½œ
                if operations_sorted[i]['timestamp'] - operations_sorted[i-1]['timestamp'] <= 2.0:
                    current_episode.append(operations_sorted[i])
                else:
                    episodes.append(current_episode)
                    current_episode = [operations_sorted[i]]
            
            episodes.append(current_episode)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            total_frames = len(operations)
            total_time = sum(len(episode) / self.actual_fps for episode in episodes)  # ä½¿ç”¨å®é™…å¸§ç‡
            avg_distance = sum(op['distance'] for op in operations) / len(operations)
            
            stats[device_id] = {
                'total_frames': total_frames,
                'total_time': total_time,
                'episodes': len(episodes),
                'avg_distance': avg_distance
            }
        
        return stats


def main():
    """ä¸»å‡½æ•°"""
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    
    # é»˜è®¤YAMLé…ç½®æ–‡ä»¶è·¯å¾„
    default_config_path = os.path.join(project_root, 'config.yaml')
    
    parser = argparse.ArgumentParser(description='è®¾å¤‡æ“ä½œæ£€æµ‹åˆ†æç³»ç»Ÿ V3')
    parser.add_argument('--config', type=str, default=default_config_path,
                       help='YAMLé…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.yaml)')
    parser.add_argument('--video', type=str, default=None,
                       help='è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„ (å¯é€‰ï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®)')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºç»“æœç›®å½•è·¯å¾„ (å¯é€‰ï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®)')
    parser.add_argument('--threshold', type=float, default=None,
                       help='æ“ä½œåˆ¤æ–­è·ç¦»é˜ˆå€¼ï¼ˆåƒç´ ï¼‰ (å¯é€‰ï¼Œä¼šè¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.config):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        return
    
    try:
        # åˆå§‹åŒ–æ£€æµ‹å™¨ï¼Œä¼ å…¥é…ç½®æ–‡ä»¶è·¯å¾„
        detector = OperationDetector(project_root, args.config)
        
        # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤å‚æ•°ï¼Œæˆ–ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
        video_path = args.video if args.video else os.path.join(project_root, detector.config['file_paths']['default_video_path'])
        output_dir = args.output if args.output else os.path.join(project_root, detector.config['file_paths']['default_output_dir'])
        
        # å¦‚æœæŒ‡å®šäº†å‘½ä»¤è¡Œå‚æ•°ï¼Œåˆ™è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
        if args.threshold is not None:
            detector.operation_threshold = args.threshold
        
        # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(video_path):
            print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return
        
        print("ğŸš€ å¯åŠ¨è®¾å¤‡æ“ä½œæ£€æµ‹åˆ†æç³»ç»Ÿ V3 - æ™ºèƒ½é…ç½®")
        print(f"ğŸ“‹ é…ç½®æ–‡ä»¶: {args.config}")
        print(f"ğŸ“¹ è¾“å…¥è§†é¢‘: {video_path}")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ğŸ“ è·ç¦»é˜ˆå€¼: {detector.operation_threshold} åƒç´ ")
        
        # å¤„ç†è§†é¢‘
        # å¤„ç†è§†é¢‘
        _ = detector.process_video(video_path, output_dir)
        
        print("âœ… åˆ†æå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
